{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "id2title = json.load(open('/home/yuqihang/projects/CoLLM/collm-datasets/book2018/id2title.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(id2title.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc   a b   d'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = \"abc   a b   d\"\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc a b d'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = re.sub(r'<[^>]+>', '', desc)\n",
    "desc = re.sub(r'\\s+', ' ', desc)\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "def get_token_len(file_name,split_nums):\n",
    "    with open(file_name,'r') as f:\n",
    "        token_len = f.readlines()\n",
    "        token_len = [int(i.strip().split(':')[-1]) for i in token_len]\n",
    "    train = token_len[:split_nums[0]]\n",
    "    valid = token_len[split_nums[0]:sum(split_nums)]\n",
    "    test = token_len[sum(split_nums):]\n",
    "    data = [\n",
    "        [\"train\", sum(train)/split_nums[0], max(train), min(train)],\n",
    "        [\"valid\", sum(valid)/split_nums[1], max(valid), min(valid)],\n",
    "        [\"test\", sum(test)/(len(token_len)-sum(split_nums)), max(test), min(test)],\n",
    "        [\"mixed\", sum(token_len)/len(token_len), max(token_len), min(token_len)]\n",
    "    ]\n",
    "    headers = [\"\", \"avg_len\", \"max_len\", \"min_len\"]\n",
    "    print(tabulate(data, headers=headers, tablefmt=\"grid\", colalign=[\"center\", \"center\", \"center\", \"center\"]))\n",
    "    return sorted(token_len)\n",
    "\n",
    "def get_text_len(file_name,split_nums):\n",
    "    with open(file_name,'r') as f:\n",
    "        text_len = f.readlines()\n",
    "        text_len = [max(eval(i.strip().split(':')[-1])) for i in text_len]\n",
    "    train = text_len[:split_nums[0]]\n",
    "    valid = text_len[split_nums[0]:sum(split_nums)]\n",
    "    test = text_len[sum(split_nums):]\n",
    "    data = [\n",
    "        [\"train\", sum(train)/split_nums[0], max(train), min(train)],\n",
    "        [\"valid\", sum(valid)/split_nums[1], max(valid), min(valid)],\n",
    "        [\"test\", sum(test)/(len(text_len)-sum(split_nums)), max(test), min(test)],\n",
    "        [\"mixed\", sum(text_len)/len(text_len), max(text_len), min(text_len)]\n",
    "    ]\n",
    "    headers = [\"\", \"avg_len\", \"max_len\", \"min_len\"]\n",
    "    print(tabulate(data, headers=headers, tablefmt=\"grid\", colalign=[\"center\", \"center\", \"center\", \"center\"]))\n",
    "    return sorted(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['ml1m_10_text_lens.txt','ml1m_10_token_lens.txt']\n",
    "split_idx = [10000,650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['ml1m_20_text_lens.txt','ml1m_20_token_lens.txt']\n",
    "split_idx = [10000,650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['ml1m_50_text_lens.txt','ml1m_50_token_lens.txt']\n",
    "split_idx = [10000,1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['book_10_text_lens.txt','book_10_token_lens.txt']\n",
    "split_idx = [10000,805]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36mml1m_50_text_lens\u001b[0m\n",
      "+-------+-----------+-----------+-----------+\n",
      "|       |  avg_len  |  max_len  |  min_len  |\n",
      "+=======+===========+===========+===========+\n",
      "| train |  1355.53  |   2039    |    354    |\n",
      "+-------+-----------+-----------+-----------+\n",
      "| valid |  1688.74  |   2004    |    543    |\n",
      "+-------+-----------+-----------+-----------+\n",
      "| test  |  1318.06  |   2074    |    357    |\n",
      "+-------+-----------+-----------+-----------+\n",
      "| mixed |  1375.3   |   2074    |    354    |\n",
      "+-------+-----------+-----------+-----------+\n",
      "\u001b[1;36mml1m_50_token_lens\u001b[0m\n",
      "+-------+-----------+-----------+-----------+\n",
      "|       |  avg_len  |  max_len  |  min_len  |\n",
      "+=======+===========+===========+===========+\n",
      "| train |  542.397  |    827    |    87     |\n",
      "+-------+-----------+-----------+-----------+\n",
      "| valid |  691.223  |    805    |    177    |\n",
      "+-------+-----------+-----------+-----------+\n",
      "| test  |  526.777  |    840    |    88     |\n",
      "+-------+-----------+-----------+-----------+\n",
      "| mixed |  551.498  |    840    |    87     |\n",
      "+-------+-----------+-----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir = \"/data/yuqihang/result/CoLLM/lens\"\n",
    "for filename in file_names:\n",
    "    filename = os.path.join(data_dir,filename)\n",
    "    print(f\"\\033[1;36m{filename.split('/')[-1].split('.')[0]}\\033[0m\")\n",
    "    if 'token' in filename:\n",
    "        seq_lens = get_token_len(filename,split_idx)\n",
    "    else:\n",
    "        seq_lens = get_text_len(filename,split_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
