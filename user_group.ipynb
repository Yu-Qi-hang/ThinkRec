{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yuqihang/envs/envs/collm/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/data/yuqihang/envs/envs/collm/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from minigpt4.models.rec_model import disabled_train\n",
    "from minigpt4.models.rec_base_models import MatrixFactorization, MF_linear,LightGCN, SASRec, Personlized_Prompt, random_mf, Soft_Prompt, RecEncoder_DIN\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "class Rec_config:\n",
    "    def __init__(self, user_num, item_num, embedding_size):\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.embedding_size = embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rec_encoder(rec_model, config):\n",
    "    if rec_model == \"MF\":\n",
    "        print(\"### rec_encoder:\", \"MF\")\n",
    "        rec_model = MatrixFactorization(config)\n",
    "    elif rec_model == \"lightgcn\":\n",
    "        print(\"### rec_encoder:\", \"lightgcn\")\n",
    "        rec_model = LightGCN(config)\n",
    "    elif rec_model == \"sasrec\":\n",
    "        print(\"### rec_encoder:\", \"sasrec\")\n",
    "        rec_model = SASRec(config)\n",
    "    elif rec_model == \"DIN\":\n",
    "        print(\"### rec_encoder:\", \"DIN\")\n",
    "        rec_model = RecEncoder_DIN(config)\n",
    "    else:\n",
    "        rec_model = None\n",
    "        warnings.warn(\" the input rec_model is not MF, LightGCN or sasrec, or DCN, we won't utilize the rec_encoder directly.\")\n",
    "    return rec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/yuqihang/projects/CoLLM/collm-datasets/bookdu/group_1\"\n",
    "train_ = pd.read_pickle(os.path.join(data_dir,\"train_ood2.pkl\"))\n",
    "valid_ = pd.read_pickle(os.path.join(data_dir,\"valid_ood2.pkl\"))\n",
    "test_ = pd.read_pickle(os.path.join(data_dir,\"test_ood2.pkl\"))\n",
    "reason_ = pd.read_pickle(os.path.join(data_dir,\"reason_ood2.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group_dir = \"/home/yuqihang/projects/CoLLM/collm-datasets/bookdu/group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = os.path.join(data_dir,\"user_group.json\")\n",
    "with open(data_group,'r') as f:\n",
    "    data_group_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_group_dict['0'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401399, 34385, 33962, 2225)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_),len(valid_),len(test_),len(reason_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249466, 21877, 22300, 2275, 10938)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx),len(valid_idx),len(test_idx),len(reason_idx),len(valid_small_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in data_group_dict:\n",
    "    data_group_dir_idx = f'{data_group_dir}_{idx}'\n",
    "    os.makedirs(data_group_dir_idx,exist_ok=True)\n",
    "    train_idx = train_[train_['uid'].isin(data_group_dict[idx])]\n",
    "    valid_idx = valid_[valid_['uid'].isin(data_group_dict[idx])]\n",
    "    # valid_small_idx = valid_small[valid_small['uid'].isin(data_group_dict[idx])]\n",
    "    valid_small_idx = valid_idx.sample(frac=0.5,random_state=2025)\n",
    "    test_idx = test_[test_['uid'].isin(data_group_dict[idx])]\n",
    "    reason_idx = reason_[reason_['uid'].isin(data_group_dict[idx])]\n",
    "    train_idx.to_pickle(os.path.join(data_group_dir_idx,\"train_ood2.pkl\"))\n",
    "    valid_idx.to_pickle(os.path.join(data_group_dir_idx,\"valid_ood2.pkl\"))\n",
    "    valid_small_idx.to_pickle(os.path.join(data_group_dir_idx,\"valid_small_ood2.pkl\"))\n",
    "    test_idx.to_pickle(os.path.join(data_group_dir_idx,\"test_ood2.pkl\"))\n",
    "    reason_idx.to_pickle(os.path.join(data_group_dir_idx,\"reason_ood2.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Rec_model\n",
      "### rec_encoder: MF\n",
      "creat MF model, user num: 22686 item num: 47059\n",
      "successfully load the pretrained model......\n",
      "freeze rec encoder\n",
      "Loading Rec_model Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2994867/1765465668.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rec_encoder.load_state_dict(torch.load(pretrained_rec, map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "print('Loading Rec_model')\n",
    "rec_model = \"MF\"\n",
    "rec_config = Rec_config(user_num=int(user_num), item_num=int(item_num), embedding_size=256)\n",
    "rec_encoder = init_rec_encoder(rec_model, rec_config)\n",
    "\n",
    "pretrained_rec = \"/data/yuqihang/result/CoLLM/checkpoints/mf/0228_booknew_best_model_d256_lr0.001_wd1e-06.pth\"\n",
    "rec_encoder.load_state_dict(torch.load(pretrained_rec, map_location=\"cpu\"))\n",
    "print(\"successfully load the pretrained model......\")\n",
    "for name, param in rec_encoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "rec_encoder = rec_encoder.eval()\n",
    "rec_encoder.train = disabled_train\n",
    "print(\"freeze rec encoder\")\n",
    "print('Loading Rec_model Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(train_.uid)\n",
    "users.extend(list(test_.uid))\n",
    "users.extend(list(valid_.uid))\n",
    "users = list(set(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeds = rec_encoder.user_encoder(torch.tensor(users)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 (使用t-SNE降维到2D)\n",
    "def visualize_clusters(embeddings, labels):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE Visualization of User Clusters')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yuqihang/envs/envs/collm/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "user_ids = users\n",
    "embeddings = user_embeds\n",
    "# 数据标准化 (对基于距离的聚类方法很重要)\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "# 执行聚类 (这里以K-Means为例)\n",
    "cluster_labels = kmeans_clustering(embeddings_scaled, max_clusters=10)\n",
    "# 将结果保存到DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'cluster': cluster_labels\n",
    "})\n",
    "visualize_clusters(embeddings_scaled, cluster_labels)\n",
    "# 查看聚类结果\n",
    "print(\"聚类结果统计:\")\n",
    "print(results['cluster'].value_counts())\n",
    "# 保存结果到CSV\n",
    "# results.to_csv('user_clusters.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
