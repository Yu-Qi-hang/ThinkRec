model:
  arch: mini_gpt4rec_v3
  model_type: pretrain_vicuna
  infer_type: native
  quest_config: {page_size: 32, max_seq_len: 1024, token_budget: 32}
  cake_config: {compress: true, cascading: true, cache_size: 1024, window_size: 32,
    tau1: 1.6, tau2: 0.4, gamma: 200.0}
  freeze_rec: true
  freeze_proj: true
  freeze_lora: true
  generate_config: {enable: true, max_len: 16}
  prompt_path: prompts/reflection_amazon.txt
  max_txt_len: 2048
  proj_token_num: 1
  proj_drop: 0
  proj_mid_times: 10
  end_sym: '###'
  prompt_template: '{}'
  llama_model: /data/yuqihang/model/Meta-Llama-3-8B-Instruct/
  user_num: -100
  item_num: -100
  ans_type: v2
  rec_model: MF
  lora_config:
    use_lora: true
    r: 8
    alpha: 16
    target_modules: [q_proj, v_proj]
    dropout: 0.05
  rec_config: {user_num: -100, item_num: -100, embedding_size: 256, pretrained_path: /data/yuqihang/result/CoLLM/checkpoints/mf/0228_booknew_best_model_d256_lr0.001_wd1e-06.pth}
  ckpt: /data/yuqihang/result/CoLLM/reproduce/20250409024/checkpoint_best.pth
datasets:
  amazon_ood:
    path: /data/yuqihang/datasets/collm-datasets/bookdu/
    data_type: default
    build_info: {seq_len: 10, cans_num: 1, use_ids: true, use_desc: true, storage: /data/yuqihang/datasets/collm-datasets/bookdu/}
run:
  task: rec_pretrain
  lr_sched: linear_warmup_cosine_lr
  init_lr: 1e-4
  min_lr: 8e-5
  warmup_lr: 1e-5
  mode: v3
  weight_decay: 1e-3
  max_epoch: 1000
  iters_per_epoch: 800
  batch_size_train: 1
  batch_size_eval: 8
  num_workers: 2
  warmup_steps: 200
  seed: 42
  output_dir: /data/yuqihang/result/CoLLM/reproduce/evals
  amp: true
  resume_ckpt_path: null
  evaluate: true
  train_splits: [train]
  valid_splits: [valid]
  test_splits: [test_small]
  device: cuda
  world_size: 1
  dist_url: env://
  distributed: false
